{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e15d10bd-d917-4eb4-94f9-3ce8d9f3d483",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "111c4ed8-1e48-4dc0-8479-e257bc3997ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca72df62-ee00-4193-b655-1fbe9615a112",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8faf989-4149-46a1-b7c2-a117e804255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_twitter_disaster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "403a0237-8bbe-4d5b-b990-2260cdf7c0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>char_length</th>\n",
       "      <th>word_length</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>['our', 'deeds', 'are', 'the', 'reason', 'of',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>['forest', 'fire', 'near', 'la', 'ronge', 'sas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "      <td>22</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>['all', 'residents', 'asked', 'to', 'shelter',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>8</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>['13000', 'people', 'receive', 'wildfires', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>16</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>['just', 'got', 'sent', 'this', 'photo', 'from...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  char_length  word_length  \\\n",
       "0       1           69           13   \n",
       "1       1           38            7   \n",
       "2       1          133           22   \n",
       "3       1           65            8   \n",
       "4       1           88           16   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  our deeds are the reason of this earthquake ma...   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  all residents asked to shelter in place are be...   \n",
       "3  13000 people receive wildfires evacuation orde...   \n",
       "4  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                              tokens  \n",
       "0  ['our', 'deeds', 'are', 'the', 'reason', 'of',...  \n",
       "1  ['forest', 'fire', 'near', 'la', 'ronge', 'sas...  \n",
       "2  ['all', 'residents', 'asked', 'to', 'shelter',...  \n",
       "3  ['13000', 'people', 'receive', 'wildfires', 'e...  \n",
       "4  ['just', 'got', 'sent', 'this', 'photo', 'from...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34904860-fb11-4319-a9e6-8dfd5b09eda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "335789cf-edc6-4d8e-979e-51e45d1b1e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'target', 'char_length',\n",
       "       'word_length', 'cleaned_text', 'tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34f8f582-04a7-4c3c-9cd9-187f6fc0870d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   id            7613 non-null   int64 \n",
      " 1   keyword       7552 non-null   object\n",
      " 2   location      5080 non-null   object\n",
      " 3   text          7613 non-null   object\n",
      " 4   target        7613 non-null   int64 \n",
      " 5   char_length   7613 non-null   int64 \n",
      " 6   word_length   7613 non-null   int64 \n",
      " 7   cleaned_text  7613 non-null   object\n",
      " 8   tokens        7613 non-null   object\n",
      "dtypes: int64(4), object(5)\n",
      "memory usage: 535.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3994aad-c8cb-40ab-871f-cf7e000ff4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>char_length</th>\n",
       "      <th>word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7613.000000</td>\n",
       "      <td>7613.00000</td>\n",
       "      <td>7613.000000</td>\n",
       "      <td>7613.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5441.934848</td>\n",
       "      <td>0.42966</td>\n",
       "      <td>101.037436</td>\n",
       "      <td>14.903586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3137.116090</td>\n",
       "      <td>0.49506</td>\n",
       "      <td>33.781325</td>\n",
       "      <td>5.732604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2734.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5408.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8146.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10873.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id      target  char_length  word_length\n",
       "count   7613.000000  7613.00000  7613.000000  7613.000000\n",
       "mean    5441.934848     0.42966   101.037436    14.903586\n",
       "std     3137.116090     0.49506    33.781325     5.732604\n",
       "min        1.000000     0.00000     7.000000     1.000000\n",
       "25%     2734.000000     0.00000    78.000000    11.000000\n",
       "50%     5408.000000     0.00000   107.000000    15.000000\n",
       "75%     8146.000000     1.00000   133.000000    19.000000\n",
       "max    10873.000000     1.00000   157.000000    31.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e4694-6f09-4c09-a9cc-192a8d696dac",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d846f-498f-4b40-933f-b24e9c9c3434",
   "metadata": {},
   "source": [
    "## 2.1. Basic Textual Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde15111-9290-454d-b5b5-a2149fe1af3c",
   "metadata": {},
   "source": [
    "### Word Frequencies (Bag of Words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0266ce12-1c78-4ec7-8ff1-b4f81d925f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_bow = count_vectorizer.fit_transform(df['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68c2a2b2-15a3-4992-ac44-a0390bc2e0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7613x15738 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 94346 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d1c44-c974-40ef-a87d-d9f375b11fd2",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency-Inverse Document Frequency):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54cd63f5-9e22-4c90-94ca-846405722917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba23187d-879a-4c73-8a5d-46fabeb8ea91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7613x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 82213 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2722d2-e505-498d-8c03-d22668300d49",
   "metadata": {},
   "source": [
    "## 2.2. BERT or Other Transformer Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f98149b-767e-44da-b063-2714bd5cb416",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"C:\\Users\\skhai\\anaconda3\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\__init__.py:27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     30\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     is_pretty_midi_available,\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Note: the following symbols are deliberately exported with `as`\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# so that mypy, pylint or other static linters can recognize them,\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# given that they are not exported using `__all__` in this file.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\__init__.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto_docstring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     25\u001b[0m     ClassAttrs,\n\u001b[0;32m     26\u001b[0m     ClassDocstring,\n\u001b[0;32m     27\u001b[0m     ImageProcessorArgs,\n\u001b[0;32m     28\u001b[0m     ModelArgs,\n\u001b[0;32m     29\u001b[0m     ModelOutputArgs,\n\u001b[0;32m     30\u001b[0m     auto_class_docstring,\n\u001b[0;32m     31\u001b[0m     auto_docstring,\n\u001b[0;32m     32\u001b[0m     get_args_doc_from_source,\n\u001b[0;32m     33\u001b[0m     parse_docstring,\n\u001b[0;32m     34\u001b[0m     set_min_indent,\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbone_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_template_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\auto_docstring.py:30\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mregex\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     25\u001b[0m     MODELS_TO_PIPELINE,\n\u001b[0;32m     26\u001b[0m     PIPELINE_TASKS_TO_SAMPLE_DOCSTRINGS,\n\u001b[0;32m     27\u001b[0m     PT_SAMPLE_DOCSTRINGS,\n\u001b[0;32m     28\u001b[0m     _prepare_output_docstrings,\n\u001b[0;32m     29\u001b[0m )\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelOutput\n\u001b[0;32m     33\u001b[0m PATH_TO_TRANSFORMERS \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mresolve() \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m AUTODOC_FILES \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfiguration_*.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodeling_*.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_extractor_*.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     44\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:51\u001b[0m\n\u001b[0;32m     47\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# required for @can_return_tuple decorator to work with torchdynamo\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_debugging_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_addition_debugger_context\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# vendored from distutils.util\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\__init__.py:281\u001b[0m\n\u001b[0;32m    277\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    279\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 281\u001b[0m     _load_dll_libraries()\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# Libraries can either be in\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# path/nvidia/lib_folder/lib or\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# path/nvidia/cuXX/lib (since CUDA 13.0) or\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m# path/lib_folder/lib\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\__init__.py:264\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    260\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    261\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    263\u001b[0m     )\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"C:\\Users\\skhai\\anaconda3\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "df['bert_embedding'] = df['cleaned_text'].apply(get_bert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc463a-6bc2-497c-a708-3bc0c20b761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c86ccc6-b462-4a40-9fe7-19bae1d223be",
   "metadata": {},
   "source": [
    "## 2.3 Sentiment Analysis: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a819e2fa-f32b-4ef4-95b4-39d9bea0903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "df['sentiment'] = df['cleaned_text'].apply(lambda x: sia.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f19b90-b34d-461b-9fab-9b26cd44892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bb589f-1a97-4da1-b56f-4d3a56aefecb",
   "metadata": {},
   "source": [
    "## 2.4- Additional Features: Hashtags, Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52150ac0-a8d9-492a-ae70-fc1487af1675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate additional features\n",
    "def extract_additional_features(text):\n",
    "    num_hashtags = len(re.findall(r\"#\\w+\", text))\n",
    "    num_mentions = len(re.findall(r\"@\\w+\", text))\n",
    "    return pd.Series([num_hashtags, num_mentions], index=['num_hashtags', 'num_mentions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b964e9-9373-4185-b92d-5c05c1724285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to both training and test sets\n",
    "additional_future = df['text'].apply(extract_additional_features)\n",
    "additional_future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fd4837-b082-4c9e-b2d3-4b23701ea46c",
   "metadata": {},
   "source": [
    "## 2.3. Combining Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7920a533-b349-497c-944b-ed3de4ce394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b661021e-0bfb-4cf6-9c90-3dcfa21745d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine BoW/TF-IDF with other features\n",
    "X_combined = hstack([X_tfidf,additional_future, np.array(df[['char_length','sentiment']])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86c849-1ceb-4a1d-bda4-0635effe3456",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0252b707-2a20-494b-a67b-035e56bc95f3",
   "metadata": {},
   "source": [
    "## Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd70a02d-5d19-4007-b9fb-144f1b0b65b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = X_combined\n",
    "y = df['target'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b8b1b-10cd-47c7-b3a9-86f600b9a244",
   "metadata": {},
   "source": [
    "## 2.5- Train Models and Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a56768-8699-4a41-97c3-09e60119eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f0fc7a-d20e-48c3-a3c0-836374db4ed8",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7081d39-5dca-4b54-b8f4-7d1f57832b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "y_pred_log = logistic_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29895757-0362-41cf-a338-58a137a7bd87",
   "metadata": {},
   "source": [
    "### 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c91de2-637d-43d4-b00a-3f7f2df37a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_model = RandomForestClassifier()\n",
    "random_model.fit(X_train, y_train)\n",
    "y_pred_ran = random_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_ran))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2910b9e2-517c-445f-95ae-217481d74e46",
   "metadata": {},
   "source": [
    "### 3. neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aa5d90-199a-44de-9871-53bf976cf957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f23c383-9ffe-4137-82ee-c86bbf3613e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neural = Sequential()\n",
    "model_neural.add(Dense(units=64, activation='relu'))\n",
    "model_neural.add(Dense(units=32, activation='relu'))\n",
    "model_neural.add(Dense(units=16, activation='relu'))\n",
    "model_neural.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model_neural.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model_neural.fit(X_train, y_train, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef177cc-2c2d-4553-938c-b40de8430710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Evaluate the model\n",
    "test_loss, test_accuracy = model_neural.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0baebc-0b4b-431c-9413-a7993bcd88fd",
   "metadata": {},
   "source": [
    "## 2.6- Hyperparameter Tuning Using Grid Search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd6e6a-90ec-47e3-8dde-d7d5b1a99ccf",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87693cc-7e9e-4472-9060-f6461f96b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "logistic_model = LogisticRegression()\n",
    "# Set up the parameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'], \n",
    "    'C': [0.01, 0.1, 1, 10, 100],                  \n",
    "    'solver': ['liblinear', 'saga'],               \n",
    "    'max_iter': [100, 200, 500]                    \n",
    "}\n",
    "# Set up GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(logistic_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a84375-d106-4193-9e7f-ccef3fa22801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7dc0a1-06f6-42ea-9c8f-70afa6935a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model after tuning\n",
    "best_logistic_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd82942-ea81-416d-a6c5-347fcb1f2600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the best model\n",
    "y_pred_log = best_logistic_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef93a4c9-16ae-4846-a1e8-ddaaa9701161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(classification_report(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4eca84-7c1b-4d3b-9f5f-081270231745",
   "metadata": {},
   "source": [
    "### 2. Random Forest Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa2611f-03a1-4c64-8098-a68488ce5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "random_model = RandomForestClassifier(random_state=42)\n",
    "# Set up the parameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],               \n",
    "    'max_depth': [None, 10, 20, 30],             \n",
    "    'min_samples_split': [2, 5, 10],              \n",
    "    'min_samples_leaf': [1, 2, 4],                \n",
    "    'max_features': ['sqrt', 'log2'],           \n",
    "    'bootstrap': [True, False]                  \n",
    "}\n",
    "# Set up GridSearchCV with cross-validation\n",
    "grid_search_ran = GridSearchCV(random_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed82f5e-d0b3-4099-8516-7de7aec82344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search_ran.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7543cb33-da9f-440f-8c3a-36b713fd0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model after tuning\n",
    "best_random_model = grid_search_ran.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bd1b9-15b3-4855-ba2a-7eaa55869786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the best model\n",
    "y_pred_ran = best_random_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1e2770-4cb0-4722-8096-3dfcc8131ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(\"Best Parameters:\", grid_search_ran.best_params_)\n",
    "print(classification_report(y_test, y_pred_ran))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151e8e1d-83a7-47c8-82d9-0018e19f4780",
   "metadata": {},
   "source": [
    "### 3. neural network model Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d7e47d-d656-40e1-8e26-bd4e5cb02eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model_neural_tue = Sequential()\n",
    "    model_neural_tue.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model_neural_tue.add(Dense(1))  # Single output for regression\n",
    "    model_neural_tue.compile(optimizer='adam', loss='mse')\n",
    "    return model_neural_tue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9201062a-cfc3-4a54-96ed-24ba3769d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasRegressor\n",
    "model_neural_tue = KerasRegressor(model=create_model, epochs=10, batch_size=10, verbose=0)\n",
    "param_grid = {\n",
    "    'batch_size': [10, 20, 40],\n",
    "    'epochs': [10, 20, 50]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc3aba-ba17-4d4b-b01d-26935d92da43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=model_neural_tue, param_distributions=param_grid, n_iter=10, error_score='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00caaaa-4a53-488f-9db0-1fe93345a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the random search\n",
    "random_result = random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f114c61e-493f-4007-b32f-3435f0bb6c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = random_result.score(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64e6766-a5a9-4857-99c4-8f851fb48d21",
   "metadata": {},
   "source": [
    "## 2.7- Save the Best Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1f80d5-7e59-4087-a38c-614f944cf631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "pickle.dump(best_logistic_model,open('regression_model.pkl','wb'))\n",
    "print(type(best_logistic_model)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6290fad0-3331-428b-9c1e-36e602098249",
   "metadata": {},
   "source": [
    "# Part 3: Model Evaluation and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7631c6e-3d78-4a12-a7d2-19b2b0ecdc90",
   "metadata": {},
   "source": [
    "## 3.1- Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46853991-10a7-4ac5-bb11-288389bf5b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred_log)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Non-Disaster', 'Disaster'], yticklabels=['Non-Disaster', 'Disaster'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643fa962-d4de-4759-893c-7e7dbd54b3ff",
   "metadata": {},
   "source": [
    "## 3.2- ROC Curve and AUC (Area Under the Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ce8d5-0ceb-419f-9709-0a96af7c09f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "y_pred_prob = best_logistic_model.predict_proba(X_test)[:, 1]  \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "auc_score = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "plt.plot(fpr, tpr, color=\"blue\", label=f\"AUC = {auc_score:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5687e7-45ce-4e43-b36b-868e74517af5",
   "metadata": {},
   "source": [
    "## 3.3- Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a60ba-b4b3-4ecb-83a8-726a7dff370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "plt.plot(recall, precision, marker='.', label=\"Precision-Recall Curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b7cd8-2913-4324-ab6b-84c0fdf8bc37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09ef84c-6eaa-4c21-9647-a11f565f811e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264eee4d-3ad4-40b8-ab8d-8bb30573a858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5576da1e-211e-474c-a1f7-8c11c9008e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
